#+TITLE: Why Good Papers Get Rejected: A Simulation of Peer Review as Noisy Signal Extraction
#+AUTHOR: Matthew D. DeAngelis
#+DATE: 2026
#+OPTIONS: toc:nil num:t
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[style=authoryear,natbib=true,backend=biber]{biblatex}
#+LATEX_HEADER: \addbibresource{~/Dropbox/BibLaTeX/library.bib}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \doublespacing

* Abstract                                                         :noexport:

Recent surveys document growing dissatisfaction with the accounting
publication process, with researchers believing acceptance rates should
nearly double and that top journals increasingly favor narrow
methodologies. The conventional explanation attributes these problems to
reviewer bias. I offer an alternative: the statistical properties of peer
review---specifically, low inter-rater reliability combined with
multi-dimensional conjunctive evaluation---produce systematic dysfunction
that disadvantages quality research even without bias. Using a simulation
calibrated to empirical reliability data (ICC = 0.34), I demonstrate that
at realistic selectivity levels, approximately 45% of deserving papers are
rejected due to measurement error alone. This error structure creates
perverse incentives: researchers maximize expected publications by producing
multiple moderate-quality papers rather than single excellent papers. My
findings reframe the conversation from "reviewers are biased" to "the system
is structurally noisy," pointing toward different remedies than those
typically proposed.

* Introduction

Recent surveys reveal growing dissatisfaction with the accounting
publication process. cite:woodComparingPublicationProcess2016 found that
accounting academics perceived very low citation patterns relative to other
disciplines and believed the review process was deteriorating.
cite:burtonPerceptionsAccountingAcademics2024 updated this survey and found
perceptions have only worsened: the rising generation of scholars holds even
more negative views than their predecessors, respondents believe acceptance
rates should nearly double, and there is widespread concern that top
journals favor narrow methodologies while underweighting practice relevance.
Perhaps most troublingly, cite:burtonWeMatterAttention2021 documented that
accounting research receives significantly less attention from policymakers
and the public than economics, finance, and other disciplines.

These patterns raise fundamental questions about whether the peer review
process is functioning effectively. The common explanation for publication
difficulties centers on reviewer /bias/---topic preferences, methodological
prejudice, or personal networks that systematically disadvantage certain
work. While bias undoubtedly exists, I offer a complementary explanation
grounded in measurement theory: the statistical properties of peer
review---specifically, low inter-rater reliability combined with
multi-dimensional conjunctive evaluation criteria---may produce systematic
dysfunction that disadvantages quality research even in the /absence/ of
bias.

The possibility that chance plays a substantial role in peer review is not
new. cite:coleChanceConsensusPeer1981 famously demonstrated that "whether or
not a proposal is funded depends in a large proportion of cases upon which
reviewers happen to be selected for it." My contribution is to formalize
this intuition and show that high false rejection rates emerge naturally
from subjectivity alone. Using an agent-based simulation calibrated to
empirical reliability data, I demonstrate that at realistic selectivity
levels, a substantial fraction of truly deserving papers are rejected due to
measurement error. This finding validates field wisdom that good papers have
roughly a 50% chance of acceptance at top journals. More troublingly, I
show that this error structure creates perverse incentives: researchers
maximize expected publications by producing multiple "very good" papers
rather than single "excellent" papers, explaining the prevalence of
incremental research and fragmentation strategies documented in the
literature.

The remainder of this paper proceeds as follows. Section [[#lit-review]]
reviews prior research on peer review reliability and positions my
contribution. Section [[#model]] develops a formal model of peer review as
noisy signal extraction. Section [[#simulation]] describes my simulation
design and calibration to empirical data. Section [[#results]] presents
results on false rejection rates, researcher incentives, and interaction
effects. Section [[#robustness]] examines robustness to modeling assumptions
and parameter uncertainty. Section [[#discussion]] discusses implications for
accounting journals and possible reforms. Section [[#conclusion]] concludes.

* Prior Research
:PROPERTIES:
:CUSTOM_ID: lit-review
:END:

** Inter-Rater Reliability in Peer Review

The empirical foundation for my simulation comes from a large literature
documenting low inter-rater reliability in peer review across disciplines.
cite:bornmannReliabilityGeneralizationStudyJournal2010 conducted a
meta-analysis of 70 reliability coefficients from 48 studies covering 19,443
manuscripts. They found a mean intraclass correlation coefficient (ICC) of
0.34 and a mean Cohen's kappa of 0.17, indicating that only about one-third
of the variance in reviewer assessments reflects true quality
differences---the remaining two-thirds is measurement error.

Critically, this low reliability was consistent across /all/ disciplines
studied, including economics, law, natural sciences, medical sciences, and
social sciences. This cross-disciplinary consistency suggests that low
reliability is an inherent feature of subjective quality assessment rather
than a problem specific to any field. cite:bornmannReliabilityGeneralizationStudyJournal2010
also found that multi-dimensional rating systems were associated with
/lower/ inter-rater reliability, a finding with important implications for
the AND-gate effect I model below.

The phenomenon of chance in peer review has been recognized for decades.
cite:coleChanceConsensusPeer1981 studied 150 NSF proposals that were each
reviewed by two independent panels and famously concluded that "whether or
not a proposal is funded depends in a large proportion of cases upon which
reviewers happen to be selected for it." This finding---that the same
proposal could be funded or rejected depending on random reviewer
assignment---remains one of the most cited demonstrations of the stochastic
nature of peer review.

** Simulation Studies of Peer Review

My work builds on a growing literature using simulation to study peer
review systems. cite:neffPeerReviewGame2006 developed a probability theory
model and characterized peer review as a "game of chance," finding that
25--50% of unsuitable papers may be published even with rigorous review
processes. They also showed that resubmission strategies---submitting
rejected papers to other journals---significantly boost publication chances,
a precursor to the strategic behavior I analyze.

cite:esareyDoesPeerReview2017 conducted a simulation study of editors,
reviewers, and the publication process in political science. His key finding
was that papers at the 80th percentile of quality face essentially coin-flip
odds of acceptance, remarkably consistent with my false negative rates
under realistic parameters. Esarey also demonstrated that conjunctive
thresholds---requiring papers to pass on multiple dimensions---amplify error
rates. My work extends this insight by formally modeling the AND-gate
effect and calibrating to empirical reliability data.

** Scholarly Consensus and Methodological Agreement

An important theoretical question is whether reviewer disagreement stems
from fundamental uncertainty about quality or from lack of consensus on
evaluation criteria. cite:hargensScholarlyConsensusJournal1988 argued that
interdisciplinary variation in rejection rates reflects variation in
/scholarly consensus/: "When scholars do not share conceptions of
appropriate research problems, theoretical approaches, or research
techniques, they tend to view each other's work as deficient and unworthy of
publication."

This insight has implications for accounting, where multiple methodological
traditions coexist (archival, experimental, analytical, qualitative) and
where cite:burtonPerceptionsAccountingAcademics2024 document concerns that
top journals favor narrow methodologies. If reviewers from different
methodological traditions disagree about what constitutes good research, the
effective ICC may be lower than the cross-disciplinary average, amplifying
the problems I document.

** The Accounting Publication Context

Several studies document growing dissatisfaction with the accounting
publication process. cite:woodComparingPublicationProcess2016 compared
publication processes across accounting, economics, finance, management,
marketing, psychology, and natural sciences. He found that accounting
academics perceived very low citation patterns relative to other
disciplines, believed the review process was deteriorating, and desired
acceptance rates roughly twice as high as current levels.

cite:burtonPerceptionsAccountingAcademics2024 updated this survey and found
perceptions have worsened since 2015. The rising generation of scholars
holds more negative views than their predecessors. Respondents believe that
top journals favor certain topic areas and methodologies, that reviewers
underweight practice relevance, and that reviewers overweight incremental
contribution and methodological rigor. The finding that new assistant
professors hold these views /more strongly/ than senior faculty suggests the
problem is intensifying rather than ameliorating.

The consequences of publication dysfunction may extend beyond individual
careers. cite:burtonWeMatterAttention2021 documented that accounting
research receives significantly less attention from policymakers and the
general public than economics, finance, and other disciplines. While
causality is difficult to establish, if peer review dysfunction steers
accounting research toward narrow, incremental contributions, this could
partly explain the field's limited external impact.

** The Research-Practice Gap

Concerns about accounting research relevance have deep roots.
cite:johnsonRelevanceLostRise1991 argued that management accounting had lost
its relevance to practitioners, becoming divorced from the information needs
of actual organizations. cite:kaplanAccountingScholarshipThat2011
subsequently called for "accounting scholarship that advances professional
knowledge and practice," arguing that the field had drifted toward technical
exercises that fail to inform either practitioners or policymakers.

More recent work has attempted to quantify this gap. cite:rajgopalIntegratingPracticeAccounting2021
assessed accounting research against three criteria: products and processes
produced, overlap with issues that concern CFOs and CEOs, and whether
academic know-how exceeds that of practitioners. He concluded that academic
research has strayed from producing work useful to either practitioners or
policymakers, conjecturing that incentive structures---particularly the
pressure to publish in elite journals---drive this problem.
cite:fraserAbundantPublicationsMinuscule2020 surveyed accounting
practitioners and found that academic research is largely ignored by the
profession, terming the situation "abundant publications but minuscule
impact."

cite:burtonRelevanceAccountingResearch2022 developed ROAR (Relevance of
Accounting Research) scores by having accounting professionals rate the
titles and abstracts of published papers. The results confirmed what many
suspected: much of what gets published in top journals is perceived as
irrelevant by those who might apply it. Combined with the attention data
from cite:burtonWeMatterAttention2021, these findings suggest a systematic
disconnect between what peer review rewards and what creates value for the
profession.

** Doctoral Socialization and Methodological Narrowing

A potential mechanism linking peer review dysfunction to research outcomes
is doctoral socialization. cite:fogartyHandThatRocks2010 examined the
American Accounting Association's Doctoral Consortium and found that "the
most important lesson about academic accounting may have been the
righteousness of the field's stratification hierarchy. In other words,
learning one's place in the US academy may be more important than any theory
or technique within academic accounting."

This socialization into hierarchy rather than substance may explain why
narrow methodologies persist despite widespread dissatisfaction. If doctoral
students learn that success requires conforming to what elite journals
accept, and if noisy peer review makes innovative work risky, then rational
career management favors incremental extensions of established approaches.
The peer review system thus perpetuates itself: new faculty adopt safe
strategies, become reviewers who favor similar work, and train the next
generation to do the same.

** Institutional Context: Accounting Journal Review Practices

Understanding the institutional structure of accounting journal review is
essential for calibrating my simulation and interpreting its implications.
The top accounting journals---The Accounting Review (TAR), Journal of
Accounting Research (JAR), and Journal of Accounting and Economics
(JAE)---differ meaningfully in their review processes, and all differ from
journals in other disciplines.

A striking feature of accounting is the small number of reviewers. TAR
typically uses two reviewers per submission, while JAR and JAE often use
just one. This stands in contrast to natural sciences, where three or more
reviewers is common, and to the medical sciences, where major journals
routinely use three to five reviewers. cite:woodComparingPublicationProcess2016
documented that accounting journals have longer times to publication than
comparable journals in economics, finance, management, and psychology,
despite using fewer reviewers.

The single-reviewer model at JAR and JAE is particularly consequential for
my analysis. With one reviewer, there is no averaging of assessments to
reduce noise---the single reviewer's noisy signal determines the paper's
fate. JAR and JAE incentivize fast turnaround by paying referees for reviews
completed within four weeks, which may improve speed but does nothing to
address reliability. The result is a system optimized for efficiency rather
than accuracy.

Desk rejection practices also vary across journals. TAR desk rejects
approximately 5% of submissions, while JAR's desk rejection rate is
approximately 15%. Papers that survive desk rejection face the full review
process, but the low desk rejection rates mean that reviewer resources are
spread across a large pool of submissions. JAR charges substantial
submission fees (currently \$750 for authors from top-tier institutions),
which may deter some submissions but does not address the fundamental
reliability problem.

The institutional structure of accounting review thus creates conditions
where low inter-rater reliability has maximum impact. With only one or two
reviewers, there is limited opportunity for noise to average out. With high
submission volumes and pressure for fast turnaround, there is limited
opportunity for careful deliberation. The simulation results I present
below should be interpreted against this institutional backdrop: accounting
journals operate with fewer reviewers than most disciplines, making them
particularly vulnerable to the measurement error problems I document.

** Interventions and Training

A natural question is whether peer review can be improved through training
or procedural reforms. cite:hesselbergReviewerTrainingImproving2023
conducted a systematic review of ten randomized controlled trials examining
reviewer training interventions. They found that training produces "little
or no improvement in peer review quality." Structured review forms similarly
fail to improve agreement. These null findings suggest that low reliability
is not primarily a problem of reviewer skill or effort---it reflects
fundamental uncertainty in quality assessment.

The Spearman-Brown prophecy formula suggests that reliability can be
improved by adding reviewers, but with sharply diminishing returns: going
from one to two reviewers halves the error variance, but going from four to
five reduces it by only 20%. Given the costs of additional reviews, there
are practical limits to this strategy. My simulation quantifies these
diminishing returns and shows that meaningful improvements require
structural changes rather than marginal additions.

** Contributions

My work makes several contributions relative to this literature. First, I
provide an integrated framework that combines empirical reliability data
(ICC = 0.34), multi-dimensional evaluation, conjunctive decision rules, and
strategic researcher behavior in a single model. Second, I calibrate to
empirical data rather than using arbitrary parameters, allowing quantitative
predictions about error rates. Third, I derive the researcher's optimal
response to noisy peer review and show that quantity dominates quality in
expected publications---a prediction that matches observed behavior
(salami-slicing, fragmentation) without requiring irrationality or bias.
Fourth, I examine interaction effects (reviewers × noise, reviewers ×
dimensions, threshold × noise) that reveal when interventions are most
effective. Finally, I apply these insights specifically to accounting,
where concerns about publication dysfunction are acute but no
accounting-specific reliability data exists.

* A Model of Peer Review
:PROPERTIES:
:CUSTOM_ID: model
:END:

** Setup

I model peer review as a noisy signal extraction problem. A paper $i$ has
true quality on $D$ dimensions:
\begin{equation}
\mathbf{Q}_i = (q_{i1}, q_{i2}, \ldots, q_{iD})
\end{equation}
where each dimension might represent interest, rigor, contribution, or other
evaluation criteria. I assume true quality on each dimension is drawn from
a normal distribution:
\begin{equation}
q_{id} \sim N(\mu, \tau^2)
\end{equation}
where $\mu$ is mean quality in the submission pool and $\tau^2$ is the
variance of true quality across papers.

** Reviewer Observation

Reviewer $j$ observes paper $i$'s quality on dimension $d$ with measurement
error:
\begin{equation}
\hat{q}_{ijd} = q_{id} + \epsilon_{ijd}
\end{equation}
where $\epsilon_{ijd} \sim N(0, \sigma^2)$ is independent noise. This noise
captures the inherent subjectivity of quality assessment---reasonable
reviewers can disagree about the same paper even without bias.

When $n$ reviewers evaluate a paper, the editor aggregates their assessments
by averaging:
\begin{equation}
\bar{q}_{id} = \frac{1}{n}\sum_{j=1}^{n}\hat{q}_{ijd} = q_{id} + \bar{\epsilon}_{id}
\end{equation}
where $\bar{\epsilon}_{id} \sim N(0, \sigma^2/n)$. Averaging reduces noise
variance by a factor of $n$, but with diminishing returns: going from 1 to 2
reviewers halves the variance, but going from 4 to 5 reduces it by only 20%.

** Calibration to Empirical Reliability

The noise parameter $\sigma$ can be calibrated to empirical inter-rater
reliability data. The intraclass correlation coefficient (ICC) is defined
as:
\begin{equation}
ICC = \frac{\tau^2}{\tau^2 + \sigma^2}
\end{equation}
This represents the proportion of observed variance attributable to true
differences between papers rather than measurement error.

cite:bornmannReliabilityGeneralizationStudyJournal2010 conducted a
meta-analysis of 70 reliability coefficients from 48 studies covering 19,443
manuscripts across disciplines. They found a mean ICC of 0.34, indicating
that only about one-third of the variance in reviewer assessments reflects
true quality differences---the remaining two-thirds is measurement error.
Critically, this low reliability was consistent across /all/ disciplines
studied, including economics, natural sciences, medical sciences, and social
sciences.

With ICC = 0.34 and assuming $\tau = 20$ (standard deviation of true quality
in the submission pool), I can solve for the noise parameter:
\begin{equation}
0.34 = \frac{400}{400 + \sigma^2} \implies \sigma^2 \approx 776 \implies \sigma \approx 28
\end{equation}
I use $\sigma = 30$ in my simulation, which is slightly conservative
(implying marginally more noise than the point estimate) and simplifies
interpretation.

** The AND-Gate Acceptance Rule

Journals typically require papers to meet standards on /multiple/
dimensions. I model this as a conjunctive (AND-gate) decision rule: a paper
is accepted if and only if its observed quality exceeds the threshold $T$ on
/all/ dimensions:
\begin{equation}
Accept_i = \mathbf{1}\left[\bar{q}_{id} \geq T \text{ for all } d \in \{1,\ldots,D\}\right]
\end{equation}

This AND-gate structure has important implications for error rates. Consider
a paper with true quality $q_{id} = T + \delta$ on all dimensions---that is,
a paper that genuinely deserves acceptance by margin $\delta$. The
probability that observed quality falls below the threshold on any single
dimension is:
\begin{equation}
P(\bar{q}_{id} < T) = \Phi\left(\frac{-\delta}{\sigma/\sqrt{n}}\right)
\end{equation}
where $\Phi(\cdot)$ is the standard normal CDF.

With AND-gating across $D$ independent dimensions, the probability of
acceptance becomes:
\begin{equation}
P(Accept) = \left[1 - \Phi\left(\frac{-\delta}{\sigma/\sqrt{n}}\right)\right]^D
\end{equation}

This multiplicative structure is what damages good papers. Even if the
probability of passing on each dimension is high, the joint probability of
passing on /all/ dimensions can be substantially lower. Noise on /any/
dimension can sink a deserving paper.

cite:bornmannReliabilityGeneralizationStudyJournal2010 found that
multi-dimensional rating systems are associated with /lower/ inter-rater
reliability, consistent with this theoretical prediction. More dimensions
mean more opportunities for noise to cause rejection.

** Correct Decision and Error Types

I define a paper as truly deserving acceptance if its true quality exceeds
the threshold on all dimensions:
\begin{equation}
ShouldAccept_i = \mathbf{1}\left[q_{id} \geq T \text{ for all } d \in \{1,\ldots,D\}\right]
\end{equation}

This yields four possible outcomes:
- *True Positive (TP):* Paper should be accepted and is accepted
- *False Positive (FP):* Paper should be rejected but is accepted
- *True Negative (TN):* Paper should be rejected and is rejected
- *False Negative (FN):* Paper should be accepted but is rejected

I focus particularly on the false negative rate (FNR):
\begin{equation}
FNR = \frac{FN}{FN + TP}
\end{equation}
This represents the proportion of deserving papers that are incorrectly
rejected---the rate at which good work is filtered out by noise rather than
quality.

* Simulation Design
:PROPERTIES:
:CUSTOM_ID: simulation
:END:

** Implementation

I implement the model described in Section [[#model]] as an agent-based
simulation. For each experimental condition, I generate 1,000 papers with
true quality drawn independently on each dimension from $N(50, 20^2)$.
Reviewers observe each dimension with independent noise drawn from $N(0,
\sigma^2)$, and observations are averaged across reviewers. The editor
applies the AND-gate acceptance rule with threshold $T$.

I vary the following parameters:
- *Number of reviewers ($n$):* 1, 2, 3, 4, 5, 7, 10
- *Reviewer noise ($\sigma$):* 15, 25, 35 (with baseline 30)
- *Number of dimensions ($D$):* 2, 3, 4
- *Threshold ($T$):* 40, 50, 60, 70

For each parameter combination, I run 20 independent replications and
report averaged results for stability.

The simulation code is implemented in Clojure and is publicly available at
https://github.com/drdebit/publication-sim for replication and extension.

** Baseline Parameters

My baseline parameters are:
- $n = 2$ reviewers (typical for accounting journals)
- $\sigma = 30$ (calibrated to ICC = 0.34)
- $D = 2$ dimensions (interest and rigor)
- $T = 50$ (moderate selectivity)

I then vary each parameter to examine main effects and interactions.

* Results
:PROPERTIES:
:CUSTOM_ID: results
:END:

** Main Finding: False Rejection Rates

#+CAPTION: Effect of Reviewer Count on Decision Quality
| Reviewers | FNR    | FPR    | Accuracy | Reliability |
|-----------+--------+--------+----------+-------------|
|         1 | 51.9%  |  17.6% |    73.8% |       55.8% |
|         2 | 45.0%  |  15.0% |    77.4% |       68.5% |
|         3 | 38.8%  |  13.2% |    80.5% |       75.3% |
|         4 | 37.6%  |  12.4% |    81.4% |       80.2% |
|         5 | 34.5%  |  11.1% |    83.1% |       83.2% |
|         7 | 30.0%  |   9.9% |    85.1% |       86.6% |
|        10 | 27.7%  |   9.3% |    86.3% |       90.2% |

My primary finding is that under realistic conditions, a substantial
proportion of deserving papers are rejected due to measurement error alone.
With 2 reviewers (the accounting norm), approximately 45% of papers that
should be accepted are rejected (FNR = 0.45).

The effect of reviewer count exhibits clear diminishing returns. Going from
1 to 2 reviewers reduces FNR by 6.9 percentage points---the largest single
improvement. Going from 2 to 5 reviewers provides another 10.5 percentage
point reduction. Beyond 5 reviewers, gains are marginal.

This finding is consistent with cite:esareyDoesPeerReview2017, who found
that papers at the 80th percentile of quality face essentially coin-flip
odds of acceptance. My simulation provides a mechanism for this finding:
the combination of low inter-rater reliability and multi-dimensional
evaluation.

** The AND-Gate Effect: Dimensions Amplify Errors

#+CAPTION: Effect of Evaluation Dimensions (AND-gating)
| Dimensions | FNR    | FPR   | Accuracy |
|------------+--------+-------+----------|
|          2 | 45.3%  | 15.3% |    77.1% |
|          3 | 60.0%  |  8.5% |    85.1% |
|          4 | 70.5%  |  4.5% |    91.6% |

Adding evaluation dimensions dramatically increases false rejection rates.
With 2 dimensions, FNR is approximately 45%. With 3 dimensions, FNR rises
to 60%. With 4 dimensions, FNR reaches 71%.

This occurs because each additional dimension is another opportunity for
noise to cause rejection. A paper must clear the threshold on /every/
dimension, so noise on /any/ dimension can sink it. This AND-gate structure
is particularly punishing for borderline-good papers.

** Selective Journals Face Higher Baseline FNR

#+CAPTION: Threshold × Noise Interaction
| Threshold | Noise SD | Accuracy | FNR    | FPR   |
|-----------+----------+----------+--------+-------|
|        40 |       15 |    82.0% | 21.1%  | 15.1% |
|        40 |       25 |    74.1% | 33.5%  | 18.8% |
|        40 |       35 |    68.3% | 42.0%  | 22.2% |
|        50 |       15 |    85.6% | 29.2%  |  9.4% |
|        50 |       25 |    79.8% | 41.5%  | 13.2% |
|        50 |       35 |    75.2% | 49.1%  | 16.6% |
|        60 |       15 |    91.9% | 34.8%  |  5.2% |
|        60 |       25 |    88.3% | 47.4%  |  8.2% |
|        60 |       35 |    84.9% | 55.2%  | 10.9% |
|        70 |       15 |    96.9% | 39.2%  |  2.1% |
|        70 |       25 |    94.8% | 48.9%  |  4.2% |
|        70 |       35 |    92.5% | 54.8%  |  6.2% |

An important finding for top accounting journals: highly selective journals
(high threshold) have /higher baseline/ false negative rates. Even with low
noise (SD = 15), a threshold of 70 yields an FNR of 39%---nearly double the
21% at a lenient threshold of 40. At high noise (SD = 35), selective
journals reach an FNR of 55%.

This occurs because selective journals are making decisions among papers
clustered near the high end of quality. Most truly deserving papers are only
marginally above threshold, making them vulnerable to downward noise. At
lenient journals, there is more separation between deserving and undeserving
papers, so noise is less likely to cause misclassification.

** Noise Helps Bad Papers, Hurts Good Papers

#+CAPTION: The lines cross at the threshold: noise systematically redistributes acceptance from deserving to undeserving papers.
#+NAME: fig:quality-noise
#+ATTR_LATEX: :width 0.85\textwidth :placement [htbp]
[[file:figures/quality-x-noise.pdf]]

Figure [[fig:quality-noise]] reveals a striking pattern: noise does not merely
add randomness---it systematically redistributes acceptance probability from
good papers to bad ones. At quality = 90 (excellent papers), acceptance
probability drops from 94% with low noise to 52% with high noise. At quality
= 50 (papers that should be rejected), acceptance probability /rises/ from
0.2% to 7%.

The lines converge near the threshold (quality = 70), where acceptance
probability is roughly 25% regardless of noise level. This means borderline
papers face a coin-flip regardless of system reliability, while the benefit
of writing excellent work is dramatically eroded by noise.

** Quality vs. Volume: Perverse Incentives

My simulation reveals that the noisy structure of peer review creates
incentives favoring quantity over quality. I compare four hypothetical
strategies for a researcher with fixed total effort (at threshold = 70, 2
reviewers, noise SD = 30):

#+CAPTION: Strategy Comparison: Quality vs. Volume
| Strategy       | Papers | Quality | P(Accept) | E[Publications] |
|----------------+--------+---------+-----------+-----------------|
| Two very good  |      2 |      78 |     40.9% |            0.82 |
| Three decent   |      3 |      68 |     20.0% |            0.60 |
| Two good       |      2 |      72 |     28.5% |            0.57 |
| One excellent  |      1 |      85 |     56.4% |            0.56 |

#+CAPTION: The "two very good" strategy dominates despite lower quality, because stochastic acceptance rewards multiple attempts.
#+NAME: fig:strategy
#+ATTR_LATEX: :width 0.8\textwidth :placement [htbp]
[[file:figures/strategy-comparison.pdf]]

The "two very good" strategy dominates "one excellent" by 46% in expected
publications, despite producing lower-quality work. This occurs because the
stochastic nature of acceptance rewards multiple attempts. Even if each
individual paper has lower acceptance probability, the portfolio effect of
multiple submissions increases expected output.

This finding helps explain several observed phenomena in accounting
research:
- The prevalence of "salami-slicing" (fragmenting research into minimal
  publishable units)
- Field wisdom to "write twice as many papers as you think you need"
- The trend toward more incremental contributions noted by
  cite:burtonPerceptionsAccountingAcademics2024

The incentive structure I document may also discourage scientific
risk-taking. cite:azoulayDoesPeerReview2025 provide evidence that peer
review penalizes researchers who pursue bold, unconventional work. My
simulation suggests a mechanism: innovative research may face higher
variance in reviewer assessments (less consensus on what "good" looks like),
amplifying the AND-gate effect and making conservative, incremental work
relatively more attractive.

** Interaction: Reviewers Help More When Noise Is High

#+CAPTION: Reviewers × Noise Interaction
| Noise SD | Reviewers | Accuracy | FNR    |
|----------+-----------+----------+--------|
|       15 |         1 |    82.1% | 35.8%  |
|       15 |         2 |    85.6% | 28.3%  |
|       15 |         3 |    88.3% | 24.3%  |
|       15 |         5 |    89.9% | 21.0%  |
|       25 |         1 |    75.8% | 49.0%  |
|       25 |         2 |    79.7% | 39.9%  |
|       25 |         3 |    81.8% | 36.1%  |
|       25 |         5 |    84.9% | 30.3%  |
|       35 |         1 |    72.6% | 54.6%  |
|       35 |         2 |    75.6% | 48.1%  |
|       35 |         3 |    78.0% | 44.0%  |
|       35 |         5 |    81.2% | 37.4%  |

I find a significant interaction between reviewer count and noise level:
additional reviewers provide greater benefit when individual reviewer noise
is high. At low noise (SD = 15), going from 1 to 5 reviewers improves
accuracy by 7.8 percentage points. At high noise (SD = 35), the same change
improves accuracy by 8.6 percentage points.

This has implications for fields with particularly subjective evaluation
criteria. If accounting reviews involve more subjectivity than, say,
statistical analysis in finance, then accounting journals would benefit
/more/ from additional reviewers than journals in more objective fields.

** Interaction: Reviewers Help More With Multi-Dimensional Evaluation

#+CAPTION: Reviewers × Dimensions Interaction
| Dimensions | Reviewers | Accuracy | FNR    | FPR   |
|------------+-----------+----------+--------+-------|
|         2D |         1 |    73.5% | 52.7%  | 18.0% |
|         2D |         2 |    77.5% | 45.4%  | 14.9% |
|         2D |         3 |    79.8% | 41.0%  | 13.1% |
|         2D |         5 |    83.3% | 34.4%  | 10.8% |
|         3D |         1 |    82.8% | 69.6%  |  9.5% |
|         3D |         2 |    85.5% | 59.6%  |  8.0% |
|         3D |         3 |    86.5% | 54.6%  |  7.6% |
|         3D |         5 |    88.5% | 45.0%  |  6.7% |
|         4D |         1 |    90.4% | 78.3%  |  5.0% |
|         4D |         2 |    91.1% | 70.0%  |  4.6% |
|         4D |         3 |    92.0% | 64.6%  |  4.1% |
|         4D |         5 |    92.9% | 56.6%  |  3.7% |

Similarly, additional reviewers provide greater benefit when papers are
evaluated on more dimensions. With 2-dimensional evaluation, going from 1 to
5 reviewers reduces FNR by 18.3 percentage points. With 4-dimensional
evaluation, the same change reduces FNR by 21.7 percentage points.

This suggests that journals with comprehensive, multi-criteria evaluation
should invest in more reviewers to compensate for the AND-gate effect.

** Supplemental Analysis: The Editor's Role

#+CAPTION: Editor as Partial Reviewer (Supplemental)
| Model              | Reviewers | Editor SD | FNR    | Accuracy |
|--------------------+-----------+-----------+--------+----------|
| Editor-as-reviewer |         1 |        50 | 52.3%  |    74.2% |
| Editor-as-reviewer |         1 |        60 | 55.6%  |    72.6% |
| Main model         |         1 |       n/a | 52.4%  |    74.1% |
| Editor-as-reviewer |         2 |        50 | 45.7%  |    77.0% |
| Editor-as-reviewer |         2 |        60 | 47.6%  |    76.1% |
| Main model         |         2 |       n/a | 45.0%  |    77.7% |
| Editor-as-reviewer |         3 |        50 | 41.0%  |    80.0% |
| Editor-as-reviewer |         3 |        60 | 43.5%  |    78.5% |
| Main model         |         3 |       n/a | 40.2%  |    79.7% |

In supplemental analysis, I examine whether editors should contribute their
own quality assessments to the reviewer average. I model the editor as an
additional reviewer with higher noise (SD = 50-60) reflecting their
shallower engagement with any single paper.

Counterintuitively, adding the editor's noisy assessment /reduces/ decision
quality. With 2 reviewers alone, accuracy is 77.7%. Adding an editor with SD
= 60 reduces accuracy to 76.1%. This occurs because averaging in a noisy
signal dilutes the more reliable reviewer assessments.

This finding supports treating the editor's role as distinct from reviewers:
editors should aggregate and interpret reviewer signals rather than adding
their own noisy quality assessment to the average.

* Robustness and Sensitivity Analysis
:PROPERTIES:
:CUSTOM_ID: robustness
:END:

My main results depend on several modeling assumptions. This section examines
sensitivity to key parameters and relaxes assumptions that may be unrealistic.
I find that while specific effect magnitudes vary, the qualitative
conclusions---high false rejection rates, perverse incentives, and diminishing
returns to reviewers---are robust across specifications.

** Sensitivity to Inter-Rater Reliability Calibration

My baseline calibration uses ICC = 0.34 from
cite:bornmannReliabilityGeneralizationStudyJournal2010. However, this estimate
comes from a meta-analysis spanning disciplines, and the true ICC for
accounting may differ. I examine sensitivity across a range of plausible
values.

#+CAPTION: Sensitivity to ICC Calibration
| ICC  | Implied σ | FNR    | Accuracy |
|------+-----------+--------+----------|
| 0.20 |      40.0 | 52.1%  |    74.2% |
| 0.25 |      34.6 | 49.3%  |    75.6% |
| 0.30 |      30.6 | 46.5%  |    76.9% |
| 0.34 |      27.9 | 43.6%  |    78.1% |
| 0.40 |      24.5 | 39.8%  |    80.1% |
| 0.45 |      22.1 | 37.6%  |    81.3% |
| 0.50 |      20.0 | 35.5%  |    82.4% |

The results show substantial sensitivity: FNR ranges from 35.5% (ICC = 0.50) to
52.1% (ICC = 0.20). However, even at the most optimistic reliability level,
more than one-third of deserving papers are rejected. At lower reliability
levels that might characterize evaluations of innovative or cross-disciplinary
work, the majority of deserving papers face rejection.

This range brackets accounting-specific scenarios. If accounting reviews
involve less consensus than the cross-disciplinary average (due to competing
methodological traditions), effective ICC may be below 0.34. Conversely, if
accounting has developed clearer quality criteria than other fields, ICC might
be higher. The qualitative conclusions---substantial false rejection and
perverse incentives---hold throughout this range.

** Aggregation Method: Mean vs. Median

My baseline model assumes editors aggregate reviewer scores by averaging. An
alternative is median aggregation, which is more robust to outlier reviewers.

#+CAPTION: Mean vs. Median Aggregation
| Reviewers | Method | Accuracy | FNR    |
|-----------+--------+----------+--------|
|         2 | Mean   |    77.9% | 43.9%  |
|         2 | Median |    76.9% | 45.0%  |
|         3 | Mean   |    80.4% | 39.2%  |
|         3 | Median |    79.9% | 40.2%  |
|         5 | Mean   |    82.8% | 34.5%  |
|         5 | Median |    80.7% | 37.5%  |

Mean aggregation slightly outperforms median across reviewer counts. With 5
reviewers, mean achieves 82.8% accuracy versus 80.7% for median---a difference
of 2.1 percentage points. This occurs because my model assumes normally
distributed noise without outliers, where averaging is statistically optimal.
In practice, if some reviewers provide "adversarial" reviews (extremely
negative assessments unrelated to quality), median aggregation could
outperform; my model does not capture this phenomenon.

** Correlated Reviewer Errors

My baseline assumes reviewer errors are independent---one reviewer's
assessment does not influence another's. This assumption may be violated if:
(1) reviewers share methodological training that biases them similarly; (2)
reviewers observe each other's assessments in sequential review; or (3)
reviewers draw on common information (e.g., the same literature) that
systematically misleads them.

I model correlated errors by generating reviewer assessments from a
multivariate normal distribution with correlation $\rho$ between reviewers.

#+CAPTION: Effect of Correlated Reviewer Errors
| Correlation | Reviewers | Accuracy | FNR    |
|-------------+-----------+----------+--------|
|        0.00 |         2 |    77.9% | 43.9%  |
|        0.00 |         3 |    80.1% | 39.9%  |
|        0.00 |         5 |    83.4% | 33.9%  |
|        0.25 |         2 |    76.9% | 46.3%  |
|        0.25 |         3 |    78.5% | 43.2%  |
|        0.25 |         5 |    80.2% | 39.9%  |
|        0.50 |         2 |    76.0% | 48.5%  |
|        0.50 |         3 |    76.9% | 46.3%  |
|        0.50 |         5 |    77.9% | 43.9%  |
|        0.75 |         2 |    75.2% | 50.5%  |
|        0.75 |         3 |    75.5% | 49.7%  |
|        0.75 |         5 |    75.5% | 49.7%  |

This analysis reveals an important finding: correlated errors eliminate the
benefit of additional reviewers. At $\rho = 0$ (independent errors), going from
2 to 5 reviewers improves accuracy from 77.9% to 83.4%---a gain of 5.5
percentage points. At $\rho = 0.75$ (highly correlated errors), the same change
yields essentially no improvement (75.2% to 75.5%).

This occurs because correlated errors do not average out. If all reviewers are
biased in the same direction---perhaps sharing a methodological worldview that
disadvantages certain research approaches---adding more reviewers from the same
pool merely confirms the shared bias. This finding has implications for
reviewer selection: diversity in reviewer backgrounds may be more valuable than
additional reviewers from similar backgrounds.

The accounting context makes correlated errors plausible. If doctoral programs
train students in similar methodologies and socialize them into similar views
of "good" research (cite:fogartyHandThatRocks2010), reviewers may share
systematic biases even without communicating. My finding suggests that
meaningful improvement requires not just more reviewers but /differently
trained/ reviewers.

** Sensitivity to Quality Variance

My baseline assumes true quality has standard deviation $\tau = 20$. This
parameter affects both the difficulty of discrimination (smaller $\tau$ means
papers are more similar) and the calibrated noise level (since ICC depends on
both $\tau$ and $\sigma$).

#+CAPTION: Sensitivity to Quality Variance (τ)
| τ  | FNR    | Accuracy |
|----+--------+----------|
| 15 | 52.9%  |    74.0% |
| 20 | 43.6%  |    78.1% |
| 25 | 38.6%  |    80.7% |
| 30 | 33.9%  |    83.1% |

Lower quality variance (τ = 15) increases FNR to 52.9%, while higher variance
(τ = 30) reduces it to 33.9%. This occurs for two reasons. First, with less
variance in true quality, papers cluster more tightly near the threshold, where
noise is most likely to cause misclassification. Second, at fixed ICC, lower
$\tau$ implies lower $\sigma$ (less absolute noise), but the relative noise
compared to quality differences remains constant.

This sensitivity has implications for interpretation. If accounting submissions
exhibit relatively homogeneous quality (most papers from qualified researchers
meeting professional standards), the effective false rejection rate would be
higher than my baseline estimate. Conversely, if quality is highly variable,
discrimination may be easier than my model suggests.

** Theoretical Considerations: Non-Normal Distributions

My model assumes normally distributed true quality and reviewer noise.
Empirical quality distributions may differ: true quality might be right-skewed
(most papers are mediocre, few are excellent) or heavy-tailed (occasional
breakthrough papers).

I do not simulate alternative distributions, but can reason about their
effects. Right-skewed quality distributions would place more papers near the
left tail, where noise is more likely to cause false /positives/ than false
negatives. This could mitigate FNR concerns for the typical paper while
exacerbating FPR concerns. Heavy-tailed noise distributions (e.g., occasional
highly discordant reviewers) would reduce the effectiveness of averaging and
potentially favor median aggregation.

The key insight is that my baseline estimates assume symmetric, well-behaved
distributions. Departures from normality could either amplify or attenuate
specific effects, but the fundamental tension between selectivity and
reliability remains.

** Summary of Robustness Findings

Across sensitivity analyses, several findings are robust:

1. *High false rejection rates persist.* Even at the most optimistic ICC (0.50)
   and highest quality variance (τ = 30), FNR exceeds 33%. Under pessimistic
   but plausible assumptions, FNR exceeds 50%.

2. *Diminishing returns to reviewers.* Going from 1 to 2 reviewers provides the
   largest benefit; going beyond 5 provides marginal improvement. This finding
   is robust across ICC values and aggregation methods.

3. *Correlated errors undermine reviewer aggregation.* If reviewers share
   systematic biases, adding reviewers provides little benefit. This suggests
   reviewer diversity may matter more than reviewer quantity.

4. *Perverse incentives persist.* The incentive to produce multiple
   moderate-quality papers rather than single excellent papers emerges from
   stochastic acceptance, which is present regardless of specific parameter
   values.

These robustness checks strengthen confidence in my main conclusions while
highlighting areas where parameter uncertainty affects quantitative estimates.

* Discussion
:PROPERTIES:
:CUSTOM_ID: discussion
:END:

** Reframing the Problem

My findings suggest a reframing of the accounting publication "problem."
The conventional view attributes publication difficulties to reviewer
bias---topic preferences, methodological prejudice, or favoritism. While
bias surely exists, my simulation demonstrates that high false rejection
rates and perverse incentives emerge naturally from the /structure/ of peer
review even without any bias.

This reframing has important implications. If the problem were primarily
bias, solutions would focus on reviewer training, blind review, or
diversifying reviewer pools. But if the problem is structural noise,
different interventions are needed. Indeed,
cite:hesselbergReviewerTrainingImproving2023 conducted a systematic review of
reviewer training interventions and found that training produces "little or
no improvement in peer review quality."

** Implications for Editors and Journals

My findings suggest several directions for editors seeking to improve decision
quality.

*Reviewer quantity and diversity.* The largest accuracy gains come from going
from 1 to 2 reviewers, but meaningful improvements continue through 3-5
reviewers. If accounting journals typically use 1-2 reviewers, there is
accuracy left on the table. However, my correlated-errors analysis suggests
that reviewer /diversity/ may matter more than reviewer /quantity/. Adding a
third reviewer from the same methodological tradition as the first two provides
less benefit than adding a reviewer with different training. Editors might
prioritize assembling methodologically diverse review teams over simply
increasing reviewer counts.

*Decision rule architecture.* Comprehensive rubrics with many evaluation
criteria may /reduce/ decision quality by creating more opportunities for noise
to reject good papers. Journals might consider compensatory rather than
conjunctive decision rules---allowing excellence on one dimension to offset
weakness on another---or reducing the number of explicit evaluation dimensions.
The current practice of requiring papers to "pass" on interest, rigor,
contribution, and writing creates four independent opportunities for noise to
sink a deserving paper.

*Transparent acknowledgment of uncertainty.* My findings suggest that roughly
half of rejection decisions for borderline-good papers are determined by noise
rather than true quality. Journals might acknowledge this uncertainty in
rejection letters, distinguishing between "clear reject" (paper unlikely to
succeed anywhere) and "unlucky reject" (paper might well have been accepted
with different reviewers). This transparency could reduce author frustration
and encourage appropriate resubmission behavior.

*Structural alternatives.* Given that training doesn't improve reliability,
journals might explore structural changes. Lottery elements for borderline
papers would make randomness explicit rather than hidden. Registered reports
separate evaluation of research questions from evaluation of results,
potentially reducing noise from hindsight bias. Multi-stage review that filters
on different dimensions sequentially rather than conjunctively could improve
accuracy for papers that clear initial screens.

*Experimentation and measurement.* Journals could measure their own reliability
by occasionally having papers reviewed by independent panels, as in
cite:coleChanceConsensusPeer1981. This would provide field-specific reliability
estimates and allow journals to evaluate whether interventions actually improve
decision quality. Without measurement, improvement is difficult.

** Implications for Researchers

My findings also have implications for researchers navigating the publication
system.

*Calibrating expectations.* The field wisdom that good papers have roughly
50-50 odds at top journals is not cynicism---it reflects the structural
properties of noisy evaluation. Researchers should calibrate expectations
accordingly, treating rejection as noisy signal rather than definitive quality
judgment. A rejection from one journal provides limited information about
likely success at another journal with different reviewers.

*Portfolio strategy.* My simulation confirms that researchers maximize
expected publications by producing multiple papers rather than concentrating
effort on single masterworks. This is not advice to produce low-quality work;
rather, it suggests that conditional on maintaining quality, distributing
effort across multiple projects dominates concentration. The "two very good
papers" strategy outperforms "one excellent paper" by 46% in expected
publications.

*Resubmission persistence.* Given high false negative rates, persistence in
resubmission is rational. A paper rejected due to noise at one journal may
succeed at another with different reviewers. cite:neffPeerReviewGame2006 showed
that resubmission strategies significantly boost publication chances. However,
this creates its own inefficiencies: reviewer effort is duplicated, and
persistence rather than quality becomes a determinant of publication.

*Risk and innovation.* My findings suggest caution about innovative or
cross-paradigm research. If such work faces higher variance in reviewer
assessments (less consensus on what "good" looks like), the AND-gate effect is
amplified. Early-career researchers facing tenure clocks may rationally choose
incremental extensions of established approaches over risky innovation. This is
individually rational but collectively suboptimal.

*Interpreting feedback.* Researchers should weight reviewer feedback by
considering the reliability of peer review. Detailed, specific critiques likely
contain signal; vague objections to "contribution" or "interest" may reflect
noise or taste differences rather than fundamental problems. The appropriate
response to rejection depends on whether the feedback identifies fixable issues
or merely reveals that reviewers were unpersuaded.

** Implications for the Profession

Beyond individual journals and researchers, my findings raise broader concerns
about knowledge production in accounting.

*Systematic bias against innovation.* If noisy peer review creates incentives
for incremental research, and if doctoral socialization reinforces these
incentives (cite:fogartyHandThatRocks2010), the profession may systematically
underinvest in innovative work. The research-practice gap documented by
cite:kaplanAccountingScholarshipThat2011 and cite:rajgopalIntegratingPracticeAccounting2021
could partly reflect a publication system that rewards safe, incremental
contributions over risky attempts to address important problems.

*Talent allocation.* High false rejection rates impose costs beyond individual
frustration. Talented researchers who experience repeated rejection may exit
academia or redirect effort toward consulting, teaching, or administration. If
rejection is partly random, some of those who exit are high-quality researchers
who drew unlucky reviewer assignments. The profession loses their potential
contributions.

*Credentialing vs. knowledge production.* My findings highlight tension
between peer review's credentialing function (certifying researcher quality for
tenure and promotion) and its knowledge production function (filtering ideas to
advance understanding). A noisy credentialing system may still "work" if
randomness averages out across researchers' careers. But a noisy knowledge
filter fails directly: good ideas are rejected, resources are wasted on
replication of review effort, and incremental work crowds out innovation.

*Collective action problems.* Individual journals face little incentive to
invest in reliability improvements. Adding reviewers is costly; measuring
reliability is effort-intensive; and journals compete on prestige rather than
decision accuracy. Improving the system may require coordinated action---from
professional associations, from funders who could require reliability
reporting, or from researchers who could collectively demand better processes.

*Alternative models.* Other fields have experimented with alternatives to
traditional peer review: open review (where reviews are published), post-publication
review (where filtering happens after publication), preprint servers (which
separate dissemination from certification), and overlay journals (which curate
rather than gatekeep). Accounting has been conservative in adopting such
innovations. My findings suggest that the costs of the traditional model may
be higher than commonly recognized, potentially justifying experimentation.

** Limitations

My simulation makes several simplifying assumptions. I model reviewer
noise as independent and identically distributed, but in reality reviewers
may have correlated biases (e.g., methodological preferences). I assume
dimensions are independent, but true quality may be correlated across
dimensions. I assume a simple averaging aggregation rule, but editors may
weight reviewers differently.

These simplifications likely make my estimates /conservative/. Correlated
reviewer errors would not average out as effectively. Correlated dimensions
would amplify the AND-gate effect. Non-uniform weighting could either help
or hurt depending on whether editors correctly identify reliable reviewers.

I also do not model the full publication ecosystem, including resubmission
to other journals, desk rejection, or strategic author behavior. Future work
could extend the simulation to capture these dynamics.

* Conclusion
:PROPERTIES:
:CUSTOM_ID: conclusion
:END:

I develop a model of peer review as noisy signal extraction and use
simulation to examine its implications for false rejection rates and
researcher incentives. Calibrating to empirical inter-rater reliability data
(ICC = 0.34), I find that under realistic conditions, approximately 45% of
deserving papers are rejected due to measurement error alone. The combination
of low reliability, multi-dimensional evaluation, and high selectivity creates
a system that systematically rejects good work---not because reviewers are
biased, but because the signal-to-noise ratio is low.

This finding reframes how to interpret and reform the publication
process. The problem is not primarily bad reviewers; it is a structurally noisy
system. Even well-intentioned actors produce high false rejection rates when
reliability is low and standards are high. Recognizing this shifts attention
from reviewer training (which doesn't work) to structural interventions: more
reviewers, different decision rules, reviewer diversity, or explicit
acknowledgment of uncertainty.

My simulation also reveals that noise creates perverse incentives. Researchers
maximize expected publications by producing multiple moderate-quality papers
rather than single excellent papers---the "two very good" strategy dominates
"one excellent" by 46% in expected output. This helps explain the prevalence of
incremental research and "salami-slicing" documented in the literature, and
suggests that observed research practices may be rational responses to
structural incentives rather than failures of scholarly values.

** Future Research Directions

Several extensions would strengthen and extend my analysis.

First, empirical validation of my calibration would be valuable.
Accounting-specific reliability data---obtained by having papers reviewed by
independent panels---would allow more precise estimates of false rejection
rates in accounting journals specifically. Such data could also test whether
reliability varies by methodology, topic area, or paper type.

Second, modeling the full publication ecosystem would capture dynamics I
abstract away. Papers rejected at one journal are often revised and resubmitted
elsewhere; modeling this process would reveal system-level outcomes (eventual
publication rates, total reviewer effort) rather than single-journal outcomes.
Strategic author behavior---targeting journals based on perceived fit, timing
submissions, or adjusting quality in response to incentives---could also be
incorporated.

Third, my model treats quality as exogenous, but in reality researchers choose
how to allocate effort. An equilibrium model in which researchers optimally
respond to publication incentives, and in which those responses affect the
distribution of submitted quality, could reveal feedback effects I do not
capture. If noisy review induces quantity-over-quality strategies, the
resulting submissions may be more homogeneous in quality, which my sensitivity
analysis suggests would /increase/ false rejection rates.

Fourth, comparative analysis across fields could test whether accounting's
institutional features---few reviewers, long review times, high rejection
rates---produce worse outcomes than alternative configurations. Natural
experiments, such as journals that changed reviewer counts or decision
processes, could provide causal evidence on the effects of structural changes.

Finally, field experiments testing interventions would move from diagnosis to
treatment. Journals could randomly assign papers to different numbers of
reviewers, different decision rules, or different reviewer selection
procedures, measuring effects on eventual outcomes (citation, replication,
practitioner impact) rather than just immediate acceptance decisions.

** Broader Implications

My analysis speaks to concerns beyond accounting. Peer review is the primary
quality filter for scholarly knowledge across disciplines, yet its reliability
is consistently low. If the problems I document are general---and the
cross-disciplinary consistency of ICC estimates suggests they are---then
scholarly communication systems worldwide may be systematically rejecting good
work and rewarding quantity over quality.

This has implications for science policy. Funders, universities, and
governments rely on publication records to allocate resources, grant tenure,
and assess research impact. If publication is substantially random, these
downstream decisions inherit that randomness. A researcher denied tenure due to
insufficient publications may simply have drawn unlucky reviewers; a field that
appears unproductive may simply face harsher review standards. Using noisy
signals for high-stakes decisions amplifies the costs of low reliability.

More fundamentally, my findings raise questions about whether peer review---in
its current form---is the right institution for filtering scholarly knowledge.
Peer review emerged in an era of scarce publication capacity, when physical
journals had page limits and printing costs. In the digital era, these
constraints have relaxed, yet I retain pre-digital filtering mechanisms. Post-publication
review, open access archives, and algorithmic curation offer alternatives that
might better balance quality control against the costs of false rejection.

I do not advocate abandoning peer review, which provides valuable functions
beyond filtering: feedback improves papers, review trains scholars in critical
evaluation, and the process itself builds scholarly community. But I do
suggest that the costs of traditional peer review may be higher than commonly
acknowledged, and that experimentation with alternatives deserves serious
consideration.

** Closing Thoughts

The frustration documented by cite:burtonPerceptionsAccountingAcademics2024 is
real. Accounting researchers believe the publication process is broken, that
good work is rejected while narrow work succeeds, and that the system rewards
the wrong behaviors. My simulation suggests these perceptions are not
paranoia---they reflect accurate intuitions about structural properties of
noisy evaluation.

But attributing dysfunction primarily to bias may misdirect reform efforts.
Bias is addressable through training, diversification, and vigilance; noise is
addressable only through structural change. If the profession spends its reform energy on
bias while the real problem is noise, efforts fail to improve the system and
may conclude---incorrectly---that the problem is intractable.

Understanding the structural sources of dysfunction is a first step toward
designing more effective knowledge production systems. I hope this paper
contributes to that understanding and motivates both measurement of actual
reliability in accounting journals and experimentation with alternative
processes. The stakes are high: if accounting research is to matter---to
practitioners, policymakers, and the public---the field needs publication systems that
reliably identify and disseminate good work. Currently, I do not have such
systems. Building them is among the most important challenges facing the
profession.

* References
:PROPERTIES:
:UNNUMBERED: t
:END:

#+LATEX: \printbibliography[heading=none]
